from typing import List, Tuple
import re
import os
import subprocess
import tempfile

from helm.benchmark.adaptation.adapter_spec import AdapterSpec
from helm.benchmark.adaptation.request_state import RequestState
from helm.benchmark.metrics.metric import Metric
from helm.benchmark.metrics.metric_name import MetricName
from helm.benchmark.metrics.metric_service import MetricService
from helm.benchmark.metrics.statistic import Stat


class FunctionalCorrectnessMetric(Metric):
    """
    Metric for evaluating functional correctness of C++ code generation.

    Measures each model's functional correctness by computing the proportion of problems
    for which its generated code passes all provided unit tests. For every generated solution,
    we compile the C++ code (using g++) and execute the full test cases. We record the
    proportions of the unit test that passes for each problem and then take the mean across
    all problems. This yields a score between 0 and 1, where 1 indicates the model produced
    flawless codes, and lower values reveal the fraction of tasks it could not solve all
    the unit test cases.
    """

    def __init__(self, compile_code: bool = True, compiler_path: str = "g++"):
        """
        Initialize the functional correctness metric.

        Args:
            compile_code: Whether to actually compile and run code (True) or simulate (False)
            compiler_path: Path to the C++ compiler (default: "g++")
        """
        self.compile_code = compile_code
        self.compiler_path = compiler_path

    def evaluate_generation(
        self,
        adapter_spec: AdapterSpec,
        request_state: RequestState,
        metric_service: MetricService,
        eval_cache_path: str,
    ) -> List[Stat]:
        """
        Evaluate LLM-generated code by running unit tests and computing pass rate.

        Returns:
            List of Stat objects containing the functional correctness score
        """
        print("\n=== FUNCTIONAL CORRECTNESS METRIC DEBUG ===")
        print(f"Instance ID: {getattr(request_state.instance, 'id', 'UNKNOWN')}")

        # Get the generated code from the request state
        if not request_state.result or not request_state.result.completions:
            print("ERROR: No output generated")
            return self._create_failure_stats("No output generated")

        generated_code = request_state.result.completions[0].text.strip()
        print(f"Generated code length: {len(generated_code)}")
        print(f"Generated code preview: {generated_code[:200]}...")

        # Get test cases from instance extra_data
        if not hasattr(request_state.instance, "extra_data") or not request_state.instance.extra_data:
            print("ERROR: No extra_data available")
            print(f"Instance attributes: {dir(request_state.instance)}")
            return self._create_failure_stats("No test data available")

        extra_data = request_state.instance.extra_data
        print(f"Extra data keys: {list(extra_data.keys())}")

        test_cases = extra_data.get("test_cases", [])
        prompt_template = extra_data.get("question_template", "")
        question_name = extra_data.get("question_name", "UNKNOWN")

        print(f"Question name: {question_name}")
        print(f"Number of test cases: {len(test_cases)}")
        print(f"Template length: {len(prompt_template)}")

        if not test_cases:
            print("ERROR: No test cases available")
            return self._create_failure_stats("No test cases available")

        print(f"First test case preview: {test_cases[0] if test_cases else 'NONE'}")

        # Run unit tests and calculate pass rate
        pass_rate = self._evaluate_functional_correctness(generated_code, test_cases, prompt_template)

        print(f"Final pass rate: {pass_rate}")
        print("=== END DEBUG ===\n")

        return [Stat(MetricName("functional_correctness")).add(pass_rate)]

    def _evaluate_functional_correctness(self, generated_code: str, test_cases: List[dict], template: str) -> float:
        """
        Evaluate the generated code against unit tests and return the proportion of tests passed.

        Args:
            generated_code: The C++ code generated by the model
            test_cases: List of test case dictionaries with 'input' and 'output' keys
            template: The question template for creating complete programs

        Returns:
            Float between 0 and 1 representing the proportion of tests passed
        """
        print("\n--- Evaluating Functional Correctness ---")
        print(f"Test cases count: {len(test_cases)}")

        if not test_cases:
            print("No test cases to evaluate")
            return 0.0

        passed_tests = 0
        total_tests = len(test_cases)

        for i, test_case in enumerate(test_cases):
            print(f"\n--- Test Case {i+1}/{total_tests} ---")
            print(f"Test case keys: {list(test_case.keys())}")
            print(f"Test input: {test_case.get('input', 'MISSING')}")
            print(f"Expected output: {test_case.get('output', 'MISSING')}")

            try:
                # Extract student code from LLM output
                print(f"LLM GENERATED CODE: {generated_code}")
                student_code = self._extract_student_code(generated_code)
                print(f"Extracted student code length: {len(student_code)}")
                print(f"Extracted code preview: {student_code[:100]}...")

                # Create complete C++ program
                complete_program = self._create_complete_program(template, student_code, test_case.get("input", ""))

                if complete_program is None:
                    print("ERROR: _create_complete_program returned None")
                    continue

                print(f"Complete program length: {len(complete_program)}")

                # Run the test
                if self.compile_code:
                    print("Running with actual compilation...")
                    success, actual_output, error = self._compile_and_run_cpp(complete_program)
                    print(f"Compilation success: {success}")
                    if not success:
                        print(f"Compilation error: {error}")
                    else:
                        print(f"Actual output: '{actual_output}'")
                else:
                    print("Running with simulation...")
                    actual_output = self._simulate_execution(student_code, test_case)
                    success = True
                    error = None
                    print(f"Simulated output: '{actual_output}'")

                # Check if test passed
                if success and actual_output is not None:
                    expected_output = test_case.get("output", "").strip()
                    test_passed = actual_output.strip() == expected_output
                    print(f"Test passed: {test_passed}")
                    print(f"Expected: '{expected_output}' | Actual: '{actual_output.strip()}'")
                    if test_passed:
                        passed_tests += 1
                else:
                    print("Test failed due to compilation/execution failure")

            except Exception as e:
                print(f"Exception in test case {i+1}: {str(e)}")
                import traceback

                traceback.print_exc()
                # Test failed due to exception
                continue

        final_rate = passed_tests / total_tests if total_tests > 0 else 0.0
        print("\n--- Final Results ---")
        print(f"Passed: {passed_tests}/{total_tests}")
        print(f"Pass rate: {final_rate}")

        return final_rate

    def _extract_student_code(self, model_code: str) -> str:
        """
        Extracts clean C++ code from model output:
        - Removes markdown
        - Trims preambles
        - Removes student's main()
        - Removes out-of-class method definitions
        - Replaces `return NULL;` with `return 0;` for int returns
        """
        import re

        # --- Step 1: Markdown extraction ---
        code_blocks = re.findall(r"```(?:c\+\+)?\n(.*?)```", model_code, flags=re.DOTALL)
        if code_blocks:
            code = "\n".join(code_blocks).strip()
            print("[Markdown extraction] Used fenced code blocks.")
        else:
            # --- Step 2: Trim non-code preamble ---
            lines = model_code.strip().splitlines()
            start_keywords = ("#include", "template", "class", "struct", "void", "int main", "using namespace")
            start_idx = 0
            for i, line in enumerate(lines):
                if any(line.strip().startswith(k) for k in start_keywords):
                    start_idx = i
                    break
            code = "\n".join(lines[start_idx:]).strip()
            print("[Fallback extraction] Trimmed preamble.")

        # --- Step 3: Remove student's main() ---
        main_match = re.search(r"\bint\s+main\s*\([^)]*\)\s*\{", code)
        if main_match:
            code = code[: main_match.start()].strip()
            print("[Code cleaner] Removed student main() function.")

        # --- Step 4: Remove out-of-class method definitions ---
        code = re.sub(r"\bArray\s*<[^>]*>\s*::\s*[\w~]+\s*\([^)]*\)\s*\{[^}]*\}", "", code, flags=re.DOTALL)
        code = re.sub(
            r"template\s*<[^>]*>\s*[\w:<>\s&*]+\s+Array\s*<[^>]*>\s*::\s*[\w~]+\s*\([^)]*\)\s*\{[^}]*\}",
            "",
            code,
            flags=re.DOTALL,
        )

        # --- Step 5: Replace NULL return with 0 ---
        code = re.sub(r"return\s+NULL\s*;", "return 0;", code)

        # --- Final touch ---
        code = code.strip()
        if "print(" in code and "void print()" not in code and "print()" not in code:
            print("⚠️ WARNING: `print()` is called in test input but not defined.")

        print(f"[Final extracted code length] {len(code)}")
        print(f"[Code preview]\n{code[:300]}...\n")
        return code


    def _create_complete_program(self, template: str, student_code: str, test_input: str) -> str:
        """Create a complete C++ program using template, student code, and test input."""
        import re

        print("\n--- Create Complete Program Debug ---")
        print(f"Template length: {len(template)}")
        print(f"Student code length: {len(student_code)}")
        print(f"Test input length: {len(test_input)}")

        # --- Step 1: Clean the raw test input ---
        clean_input = re.sub(r"STD input:\s*$", "", test_input).strip()

        # --- Step 2: Fix mismatches from test input ---
        # Fix incorrect constructor calls: reduce args from 2 to 1
        clean_input = re.sub(r"new\s+Array\s*<([^>]+)>\s*\([^,]+,\s*[^)]+\)", r"new Array<\1>(200)", clean_input)

        # Remove all `print()` calls
        clean_input = re.sub(r"\b\w+->print\(\);\s*", "", clean_input)

        print(f"Cleaned input: '{clean_input}'")

        # --- Step 3: Use template structure ---
        if "{{ STUDENT_ANSWER }}" in template:
            print("Using template with STUDENT_ANSWER placeholder")

            complete_code = template.replace("{{ STUDENT_ANSWER }}", student_code)

            template_loop_pattern = r"{%\s*for\s+TEST\s+in\s+TESTCASES\s*%}.*?{%\s*endfor\s*%}"
            test_block = f"""{{
            {clean_input};
           }}"""

            complete_code = re.sub(template_loop_pattern, test_block, complete_code, flags=re.DOTALL)
            complete_code = re.sub(r"{%.*?%}", "", complete_code, flags=re.DOTALL)
            complete_code = re.sub(r"{{.*?}}", "", complete_code, flags=re.DOTALL)

            print(f"Template-based complete code length: {len(complete_code)}")
            return complete_code

        # --- Step 4: Default fallback structure ---
        print("No template found. Using fallback layout.")
        return f"""#include <iostream>
    #include <vector>
    #include <algorithm>
    using namespace std;

    {student_code}

    int main() {{
        {clean_input};
        return 0;
    }}"""

    def _compile_and_run_cpp(self, code: str) -> Tuple[bool, str, str]:
        """Compile and run C++ code, return (success, stdout, stderr)."""
        print("\n--- Compile and Run (Functional Correctness) ---")
        print(f"Code length: {len(code)}")

        try:
            # Set environment to avoid HuggingFace tokenizer warnings
            env = os.environ.copy()
            env["TOKENIZERS_PARALLELISM"] = "false"

            # Create temporary files
            with tempfile.NamedTemporaryFile(mode="w", suffix=".cpp", delete=False) as cpp_file:
                cpp_file.write(code)
                cpp_file_path = cpp_file.name

            print(f"Created temp file: {cpp_file_path}")

            # Use .out extension instead of .exe for better cross-platform compatibility
            exe_file_path = cpp_file_path.replace(".cpp", ".out")

            # Compile the code
            compile_cmd = [self.compiler_path, "-std=c++17", "-o", exe_file_path, cpp_file_path]
            print(f"Compile command: {' '.join(compile_cmd)}")

            compile_result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=30, env=env)

            print(f"Compile return code: {compile_result.returncode}")
            if compile_result.stderr:
                print(f"Compile stderr: {compile_result.stderr}")

            if compile_result.returncode != 0:
                return False, "", f"Compilation Error: {compile_result.stderr}"

            # Run the executable
            print(f"Running executable: {exe_file_path}")
            run_result = subprocess.run([exe_file_path], capture_output=True, text=True, timeout=10, env=env)

            print(f"Run return code: {run_result.returncode}")
            print(f"Run stdout: '{run_result.stdout}'")

            return True, run_result.stdout.strip(), run_result.stderr.strip()

        except subprocess.TimeoutExpired:
            print("ERROR: Execution timed out")
            return False, "", "Execution timed out"
        except FileNotFoundError as e:
            print(f"ERROR: Compiler not found: {e}")
            return False, "", f"Compiler '{self.compiler_path}' not found. Please install g++ or specify correct path."
        except Exception as e:
            print(f"ERROR: Exception during compilation: {e}")
            return False, "", f"Error: {str(e)}"
        finally:
            # Clean up temporary files
            try:
                if "cpp_file_path" in locals():
                    os.unlink(cpp_file_path)
                    print(f"Cleaned up: {cpp_file_path}")
                if "exe_file_path" in locals() and os.path.exists(exe_file_path):
                    os.unlink(exe_file_path)
                    print(f"Cleaned up: {exe_file_path}")
            except Exception as e:
                print(f"Cleanup error: {e}")

    def _simulate_execution(self, student_code: str, test_case: dict) -> str:
        """
        Simulate code execution for testing without actual compilation.

        Args:
            student_code: The extracted student code
            test_case: Dictionary containing test input and expected output

        Returns:
            Simulated output string
        """
        test_input = test_case.get("input", "")
        expected_output = test_case.get("output", "")

        # Simulation for reverse function
        if "reverse" in student_code.lower() and "arr[]" in test_input:
            # Extract array from test input
            array_match = re.search(r"int arr\[\] = \{([^}]+)\}", test_input)
            if array_match:
                try:
                    numbers = [n.strip() for n in array_match.group(1).split(",")]
                    reversed_numbers = numbers[::-1]
                    return ", ".join(reversed_numbers)
                except Exception as e:
                    print(f"Error simulating reverse function: {e}")

        # Simulation for factorial function
        if "factorial" in student_code.lower() and "factorial(" in test_input:
            # Extract number from factorial call
            factorial_match = re.search(r"factorial\((\d+)\)", test_input)
            if factorial_match:
                try:
                    n = int(factorial_match.group(1))
                    result = 1
                    for i in range(1, n + 1):
                        result *= i
                    return str(result)
                except Exception as e:
                    print(f"Error simulating factorial function: {e}")

        # Simulation for BST enlarge (more complex)
        if "enlarge" in student_code.lower() and "BTNode" in student_code:
            # This would need more sophisticated simulation
            # For now, return expected output
            return expected_output

        # Default: return expected output (perfect simulation)
        return expected_output

    def _create_failure_stats(self, error_message: str) -> List[Stat]:
        """
        Create default statistics for failure cases.

        Args:
            error_message: Description of the failure

        Returns:
            List containing a single Stat with 0.0 functional correctness score
        """
        print(f"METRIC FAILURE: {error_message}")
        return [Stat(MetricName("functional_correctness")).add(0.0)]
