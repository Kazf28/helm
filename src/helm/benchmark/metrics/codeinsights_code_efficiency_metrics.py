from typing import List, Tuple, Optional
import re
import os
import subprocess
import tempfile
import time
import statistics
from dataclasses import dataclass

from helm.benchmark.adaptation.adapter_spec import AdapterSpec
from helm.benchmark.adaptation.request_state import RequestState
from helm.benchmark.metrics.metric import Metric
from helm.benchmark.metrics.metric_name import MetricName
from helm.benchmark.metrics.metric_service import MetricService
from helm.benchmark.metrics.statistic import Stat


@dataclass
class RuntimeResult:
    """Container for runtime measurement results."""

    success: bool
    avg_runtime: float
    std_runtime: float
    min_runtime: float
    max_runtime: float
    runs_completed: int
    error_message: str = ""


class FunctionalCorrectnessMetric(Metric):
    """
    Metric for evaluating functional correctness of C++ code generation.

    Measures each model's functional correctness by computing the proportion of problems
    for which its generated code passes all provided unit tests. For every generated solution,
    we compile the C++ code (using g++) and execute the full test cases. We record the
    proportions of the unit test that passes for each problem and then take the mean across
    all problems. This yields a score between 0 and 1, where 1 indicates the model produced
    flawless codes, and lower values reveal the fraction of tasks it could not solve all
    the unit test cases.
    """

    def __init__(self, compile_code: bool = True, compiler_path: str = "g++"):
        """
        Initialize the functional correctness metric.

        Args:
            compile_code: Whether to actually compile and run code (True) or simulate (False)
            compiler_path: Path to the C++ compiler (default: "g++")
        """
        self.compile_code = compile_code
        self.compiler_path = compiler_path

    def evaluate_generation(
        self,
        adapter_spec: AdapterSpec,
        request_state: RequestState,
        metric_service: MetricService,
        eval_cache_path: str,
    ) -> List[Stat]:
        """
        Evaluate LLM-generated code by running unit tests and computing pass rate.

        Returns:
            List of Stat objects containing the functional correctness score
        """
        print("\n=== FUNCTIONAL CORRECTNESS METRIC DEBUG ===")
        print(f"Instance ID: {getattr(request_state.instance, 'id', 'UNKNOWN')}")

        # Get the generated code from the request state
        if not request_state.result or not request_state.result.completions:
            print("ERROR: No output generated")
            return self._create_failure_stats("No output generated")

        generated_code = request_state.result.completions[0].text.strip()
        print(f"Generated code length: {len(generated_code)}")
        print(f"Generated code preview: {generated_code[:200]}...")

        # Get test cases from instance extra_data
        if not hasattr(request_state.instance, "extra_data") or not request_state.instance.extra_data:
            print("ERROR: No extra_data available")
            print(f"Instance attributes: {dir(request_state.instance)}")
            return self._create_failure_stats("No test data available")

        extra_data = request_state.instance.extra_data
        print(f"Extra data keys: {list(extra_data.keys())}")

        test_cases = extra_data.get("test_cases", [])
        prompt_template = extra_data.get("question_template", "")
        question_name = extra_data.get("question_name", "UNKNOWN")

        print(f"Question name: {question_name}")
        print(f"Number of test cases: {len(test_cases)}")
        print(f"Template length: {len(prompt_template)}")

        if not test_cases:
            print("ERROR: No test cases available")
            return self._create_failure_stats("No test cases available")

        print(f"First test case preview: {test_cases[0] if test_cases else 'NONE'}")

        # Run unit tests and calculate pass rate
        pass_rate = self._evaluate_functional_correctness(generated_code, test_cases, prompt_template)

        print(f"Final pass rate: {pass_rate}")
        print("=== END DEBUG ===\n")

        return [Stat(MetricName("functional_correctness")).add(pass_rate)]

    def _evaluate_functional_correctness(self, generated_code: str, test_cases: List[dict], template: str) -> float:
        """
        Evaluate the generated code against unit tests and return the proportion of tests passed.

        Args:
            generated_code: The C++ code generated by the model
            test_cases: List of test case dictionaries with 'input' and 'output' keys
            template: The question template for creating complete programs

        Returns:
            Float between 0 and 1 representing the proportion of tests passed
        """
        print("\n--- Evaluating Functional Correctness ---")
        print(f"Test cases count: {len(test_cases)}")

        if not test_cases:
            print("No test cases to evaluate")
            return 0.0

        passed_tests = 0
        total_tests = len(test_cases)

        for i, test_case in enumerate(test_cases):
            print(f"\n--- Test Case {i+1}/{total_tests} ---")
            print(f"Test case keys: {list(test_case.keys())}")
            print(f"Test input: {test_case.get('input', 'MISSING')}")
            print(f"Expected output: {test_case.get('output', 'MISSING')}")

            try:
                # Extract student code from LLM output
                print(f"LLM GENERATED CODE: {generated_code}")
                student_code = self._extract_student_code(generated_code)
                print(f"Extracted student code length: {len(student_code)}")
                print(f"Extracted code preview: {student_code[:100]}...")

                # Create complete C++ program
                complete_program = self._create_complete_program(template, student_code, test_case.get("input", ""))

                if complete_program is None:
                    print("ERROR: _create_complete_program returned None")
                    continue

                print(f"Complete program length: {len(complete_program)}")

                # Run the test
                if self.compile_code:
                    print("Running with actual compilation...")
                    success, actual_output, error = self._compile_and_run_cpp(complete_program)
                    print(f"Compilation success: {success}")
                    if not success:
                        print(f"Compilation error: {error}")
                    else:
                        print(f"Actual output: '{actual_output}'")
                else:
                    print("Running with simulation...")
                    actual_output = self._simulate_execution(student_code, test_case)
                    success = True
                    error = None
                    print(f"Simulated output: '{actual_output}'")

                # Check if test passed
                if success and actual_output is not None:
                    expected_output = test_case.get("output", "").strip()
                    test_passed = actual_output.strip() == expected_output
                    print(f"Test passed: {test_passed}")
                    print(f"Expected: '{expected_output}' | Actual: '{actual_output.strip()}'")
                    if test_passed:
                        passed_tests += 1
                else:
                    print("Test failed due to compilation/execution failure")

            except Exception as e:
                print(f"Exception in test case {i+1}: {str(e)}")
                import traceback

                traceback.print_exc()
                # Test failed due to exception
                continue

        final_rate = passed_tests / total_tests if total_tests > 0 else 0.0
        print("\n--- Final Results ---")
        print(f"Passed: {passed_tests}/{total_tests}")
        print(f"Pass rate: {final_rate}")

        return final_rate

    def _extract_student_code(self, model_code: str) -> str:
        """
        Extracts clean C++ code from model output:
        - Removes markdown
        - Trims preambles
        - Removes student's main()
        - Removes out-of-class method definitions
        - Replaces `return NULL;` with `return 0;` for int returns
        """
        import re

        # --- Step 1: Markdown extraction ---
        code_blocks = re.findall(r"```(?:c\+\+)?\n(.*?)```", model_code, flags=re.DOTALL)
        if code_blocks:
            code = "\n".join(code_blocks).strip()
            print("[Markdown extraction] Used fenced code blocks.")
        else:
            # --- Step 2: Trim non-code preamble ---
            lines = model_code.strip().splitlines()
            start_keywords = ("#include", "template", "class", "struct", "void", "int main", "using namespace")
            start_idx = 0
            for i, line in enumerate(lines):
                if any(line.strip().startswith(k) for k in start_keywords):
                    start_idx = i
                    break
            code = "\n".join(lines[start_idx:]).strip()
            print("[Fallback extraction] Trimmed preamble.")

        # --- Step 3: Remove student's main() ---
        main_match = re.search(r"\bint\s+main\s*\([^)]*\)\s*\{", code)
        if main_match:
            code = code[: main_match.start()].strip()
            print("[Code cleaner] Removed student main() function.")

        # --- Step 4: Remove out-of-class method definitions ---
        code = re.sub(r"\bArray\s*<[^>]*>\s*::\s*[\w~]+\s*\([^)]*\)\s*\{[^}]*\}", "", code, flags=re.DOTALL)
        code = re.sub(
            r"template\s*<[^>]*>\s*[\w:<>\s&*]+\s+Array\s*<[^>]*>\s*::\s*[\w~]+\s*\([^)]*\)\s*\{[^}]*\}",
            "",
            code,
            flags=re.DOTALL,
        )

        # --- Step 5: Replace NULL return with 0 ---
        code = re.sub(r"return\s+NULL\s*;", "return 0;", code)

        # --- Final touch ---
        code = code.strip()
        if "print(" in code and "void print()" not in code and "print()" not in code:
            print("⚠️ WARNING: `print()` is called in test input but not defined.")

        print(f"[Final extracted code length] {len(code)}")
        print(f"[Code preview]\n{code[:300]}...\n")
        return code

    def _create_complete_program(self, template: str, student_code: str, test_input: str) -> str:
        """Create a complete C++ program using template, student code, and test input."""
        import re

        print("\n--- Create Complete Program Debug ---")
        print(f"Template length: {len(template)}")
        print(f"Student code length: {len(student_code)}")
        print(f"Test input length: {len(test_input)}")

        # --- Step 1: Clean the raw test input ---
        clean_input = re.sub(r"STD input:\s*$", "", test_input).strip()

        # --- Step 2: Fix mismatches from test input ---
        # Fix incorrect constructor calls: reduce args from 2 to 1
        clean_input = re.sub(r"new\s+Array\s*<([^>]+)>\s*\([^,]+,\s*[^)]+\)", r"new Array<\1>(200)", clean_input)

        # Remove all `print()` calls
        clean_input = re.sub(r"\b\w+->print\(\);\s*", "", clean_input)

        print(f"Cleaned input: '{clean_input}'")

        # --- Step 3: Use template structure ---
        if "{{ STUDENT_ANSWER }}" in template:
            print("Using template with STUDENT_ANSWER placeholder")

            complete_code = template.replace("{{ STUDENT_ANSWER }}", student_code)

            template_loop_pattern = r"{%\s*for\s+TEST\s+in\s+TESTCASES\s*%}.*?{%\s*endfor\s*%}"
            test_block = f"""{{
            {clean_input};
           }}"""

            complete_code = re.sub(template_loop_pattern, test_block, complete_code, flags=re.DOTALL)
            complete_code = re.sub(r"{%.*?%}", "", complete_code, flags=re.DOTALL)
            complete_code = re.sub(r"{{.*?}}", "", complete_code, flags=re.DOTALL)

            print(f"Template-based complete code length: {len(complete_code)}")
            return complete_code

        # --- Step 4: Default fallback structure ---
        print("No template found. Using fallback layout.")
        return f"""#include <iostream>
    #include <vector>
    #include <algorithm>
    using namespace std;

    {student_code}

    int main() {{
        {clean_input};
        return 0;
    }}"""

    def _compile_and_run_cpp(self, code: str) -> Tuple[bool, str, str]:
        """Compile and run C++ code, return (success, stdout, stderr)."""
        print("\n--- Compile and Run (Functional Correctness) ---")
        print(f"Code length: {len(code)}")

        try:
            # Set environment to avoid HuggingFace tokenizer warnings
            env = os.environ.copy()
            env["TOKENIZERS_PARALLELISM"] = "false"

            # Create temporary files
            with tempfile.NamedTemporaryFile(mode="w", suffix=".cpp", delete=False) as cpp_file:
                cpp_file.write(code)
                cpp_file_path = cpp_file.name

            print(f"Created temp file: {cpp_file_path}")

            # Use .out extension instead of .exe for better cross-platform compatibility
            exe_file_path = cpp_file_path.replace(".cpp", ".out")

            # Compile the code
            compile_cmd = [self.compiler_path, "-std=c++17", "-o", exe_file_path, cpp_file_path]
            print(f"Compile command: {' '.join(compile_cmd)}")

            compile_result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=30, env=env)

            print(f"Compile return code: {compile_result.returncode}")
            if compile_result.stderr:
                print(f"Compile stderr: {compile_result.stderr}")

            if compile_result.returncode != 0:
                return False, "", f"Compilation Error: {compile_result.stderr}"

            # Run the executable
            print(f"Running executable: {exe_file_path}")
            run_result = subprocess.run([exe_file_path], capture_output=True, text=True, timeout=10, env=env)

            print(f"Run return code: {run_result.returncode}")
            print(f"Run stdout: '{run_result.stdout}'")

            return True, run_result.stdout.strip(), run_result.stderr.strip()

        except subprocess.TimeoutExpired:
            print("ERROR: Execution timed out")
            return False, "", "Execution timed out"
        except FileNotFoundError as e:
            print(f"ERROR: Compiler not found: {e}")
            return False, "", f"Compiler '{self.compiler_path}' not found. Please install g++ or specify correct path."
        except Exception as e:
            print(f"ERROR: Exception during compilation: {e}")
            return False, "", f"Error: {str(e)}"
        finally:
            # Clean up temporary files
            try:
                if "cpp_file_path" in locals():
                    os.unlink(cpp_file_path)
                    print(f"Cleaned up: {cpp_file_path}")
                if "exe_file_path" in locals() and os.path.exists(exe_file_path):
                    os.unlink(exe_file_path)
                    print(f"Cleaned up: {exe_file_path}")
            except Exception as e:
                print(f"Cleanup error: {e}")

    def _simulate_execution(self, student_code: str, test_case: dict) -> str:
        """
        Simulate code execution for testing without actual compilation.

        Args:
            student_code: The extracted student code
            test_case: Dictionary containing test input and expected output

        Returns:
            Simulated output string
        """
        test_input = test_case.get("input", "")
        expected_output = test_case.get("output", "")

        # Simulation for reverse function
        if "reverse" in student_code.lower() and "arr[]" in test_input:
            # Extract array from test input
            array_match = re.search(r"int arr\[\] = \{([^}]+)\}", test_input)
            if array_match:
                try:
                    numbers = [n.strip() for n in array_match.group(1).split(",")]
                    reversed_numbers = numbers[::-1]
                    return ", ".join(reversed_numbers)
                except Exception as e:
                    print(f"Error simulating reverse function: {e}")

        # Simulation for factorial function
        if "factorial" in student_code.lower() and "factorial(" in test_input:
            # Extract number from factorial call
            factorial_match = re.search(r"factorial\((\d+)\)", test_input)
            if factorial_match:
                try:
                    n = int(factorial_match.group(1))
                    result = 1
                    for i in range(1, n + 1):
                        result *= i
                    return str(result)
                except Exception as e:
                    print(f"Error simulating factorial function: {e}")

        # Simulation for BST enlarge (more complex)
        if "enlarge" in student_code.lower() and "BTNode" in student_code:
            # This would need more sophisticated simulation
            # For now, return expected output
            return expected_output

        # Default: return expected output (perfect simulation)
        return expected_output

    def _create_failure_stats(self, error_message: str) -> List[Stat]:
        """
        Create default statistics for failure cases.

        Args:
            error_message: Description of the failure

        Returns:
            List containing a single Stat with 0.0 functional correctness score
        """
        print(f"METRIC FAILURE: {error_message}")
        return [Stat(MetricName("functional_correctness")).add(0.0)]


class RuntimeEfficiencyMetric(Metric):
    """
    Metric for evaluating runtime efficiency alignment between LLM-generated code and student code.

    This metric compares the runtime performance of LLM-generated code with the original student code
    when both solutions are functionally correct. It helps determine if LLMs preserve the efficiency
    characteristics of student code or over-optimize solutions.
    """

    def __init__(
        self,
        compile_code: bool = True,
        compiler_path: str = "g++",
        num_runtime_runs: int = 5,
        timeout_seconds: int = 10,
    ):
        """
        Initialize the runtime efficiency metric.

        Args:
            compile_code: Whether to actually compile and run code (True) or simulate (False)
            compiler_path: Path to the C++ compiler (default: "g++")
            num_runtime_runs: Number of times to run each solution for averaging (default: 5)
            timeout_seconds: Maximum execution time per run (default: 10)
        """
        self.compile_code = compile_code
        self.compiler_path = compiler_path
        self.num_runtime_runs = num_runtime_runs
        self.timeout_seconds = timeout_seconds

    def evaluate_generation(
        self,
        adapter_spec: AdapterSpec,
        request_state: RequestState,
        metric_service: MetricService,
        eval_cache_path: str,
    ) -> List[Stat]:
        """
        Evaluate runtime efficiency by comparing LLM code with student reference code.

        Returns:
            List of Stat objects containing runtime efficiency metrics
        """
        print("\n=== RUNTIME EFFICIENCY METRIC DEBUG ===")
        print(f"Instance ID: {getattr(request_state.instance, 'id', 'UNKNOWN')}")

        # Get the generated code from the request state
        if not request_state.result or not request_state.result.completions:
            print("ERROR: No output generated")
            return self._create_failure_stats("No output generated")

        llm_code = request_state.result.completions[0].text.strip()
        print(f"LLM code length: {len(llm_code)}")

        # Get the student reference code
        if not request_state.instance.references:
            print("ERROR: No reference code available")
            return self._create_failure_stats("No reference code available")

        student_code = request_state.instance.references[0].output.text.strip()
        print(f"Student code length: {len(student_code)}")

        # Get test cases and template from instance extra_data
        if not hasattr(request_state.instance, "extra_data") or not request_state.instance.extra_data:
            print("ERROR: No extra_data available")
            return self._create_failure_stats("No test data available")

        extra_data = request_state.instance.extra_data
        test_cases = extra_data.get("test_cases", [])
        prompt_template = extra_data.get("question_template", "")
        question_name = extra_data.get("question_name", "UNKNOWN")

        print(f"Question name: {question_name}")
        print(f"Number of test cases: {len(test_cases)}")

        if not test_cases:
            print("ERROR: No test cases available")
            return self._create_failure_stats("No test cases available")

        # First, verify both solutions are functionally correct
        llm_correctness = self._verify_functional_correctness(llm_code, test_cases, prompt_template)
        student_correctness = self._verify_functional_correctness(student_code, test_cases, prompt_template)

        print(f"LLM correctness: {llm_correctness}")
        print(f"Student correctness: {student_correctness}")

        # Measure runtime for both solutions regardless of correctness
        # This allows us to analyze runtime even when one solution fails
        llm_runtime = self._measure_runtime(llm_code, test_cases, prompt_template, "LLM")
        student_runtime = self._measure_runtime(student_code, test_cases, prompt_template, "Student")

        print(f"LLM runtime result: {llm_runtime}")
        print(f"Student runtime result: {student_runtime}")

        # Calculate efficiency metrics (handles cases where one or both fail)
        stats = self._calculate_efficiency_metrics(llm_runtime, student_runtime, llm_correctness, student_correctness)

        print(f"Final efficiency stats: {[stat.name.name for stat in stats]}")
        print("=== END RUNTIME EFFICIENCY DEBUG ===\n")

        return stats

    def _verify_functional_correctness(self, code: str, test_cases: List[dict], template: str) -> bool:
        """
        Verify that the code passes all test cases.

        Args:
            code: The code to verify
            test_cases: List of test case dictionaries
            template: The question template

        Returns:
            True if all tests pass, False otherwise
        """
        print("\n--- Verifying Functional Correctness ---")

        if not test_cases:
            return False

        for i, test_case in enumerate(test_cases):
            try:
                # Extract and prepare code
                student_code = self._extract_student_code(code)
                complete_program = self._create_complete_program(template, student_code, test_case.get("input", ""))

                if complete_program is None:
                    print(f"Test {i+1}: Failed to create complete program")
                    return False

                # Run the test with actual compilation
                success, actual_output, error = self._compile_and_run_cpp(complete_program)
                if not success:
                    print(f"Test {i+1}: Compilation/execution failed - {error}")
                    return False

                # Check output
                expected_output = test_case.get("output", "").strip()
                if actual_output.strip() != expected_output:
                    print(
                        f"Test {i+1}: Output mismatch - Expected: '{expected_output}', Got: '{actual_output.strip()}'"
                    )
                    return False

            except Exception as e:
                print(f"Test {i+1}: Exception - {str(e)}")
                return False

        print("All tests passed - code is functionally correct")
        return True

    def _measure_runtime(self, code: str, test_cases: List[dict], template: str, code_type: str) -> RuntimeResult:
        """
        Measure the runtime of code across multiple test cases and runs.

        This method attempts to measure runtime even for incorrect solutions,
        as long as they compile and run without crashing.

        Args:
            code: The code to measure
            test_cases: List of test case dictionaries
            template: The question template
            code_type: Description for logging ("LLM" or "Student")

        Returns:
            RuntimeResult containing timing statistics
        """
        print(f"\n--- Measuring Runtime for {code_type} Code ---")

        all_runtimes = []
        student_code = self._extract_student_code(code)

        # Run each test case multiple times to get average runtime
        for i, test_case in enumerate(test_cases):
            try:
                complete_program = self._create_complete_program(template, student_code, test_case.get("input", ""))
                if complete_program is None:
                    print(f"Test case {i+1}: Failed to create complete program")
                    continue

                # Compile once for this test case
                exe_path = self._compile_cpp_program(complete_program)
                if exe_path is None:
                    print(f"Test case {i+1}: Compilation failed")
                    continue

                # Run multiple times for timing (regardless of correctness)
                test_runtimes = []
                for run in range(self.num_runtime_runs):
                    runtime = self._time_execution(exe_path)
                    if runtime is not None:
                        test_runtimes.append(runtime)
                        print(f"Test case {i+1}, run {run+1}: {runtime:.6f}s")

                if test_runtimes:
                    all_runtimes.extend(test_runtimes)
                    print(f"Test case {i+1}: {len(test_runtimes)} successful runs")
                else:
                    print(f"Test case {i+1}: No successful runs")

                # Cleanup
                try:
                    os.unlink(exe_path)
                except Exception as e:
                    print(f"Error cleaning up executable: {e}")

            except Exception as e:
                print(f"Error measuring runtime for test case {i+1}: {e}")
                continue

        if not all_runtimes:
            return RuntimeResult(
                success=False,
                avg_runtime=0.0,
                std_runtime=0.0,
                min_runtime=0.0,
                max_runtime=0.0,
                runs_completed=0,
                error_message="No successful runtime measurements",
            )

        # Calculate statistics
        avg_runtime = statistics.mean(all_runtimes)
        std_runtime = statistics.stdev(all_runtimes) if len(all_runtimes) > 1 else 0.0
        min_runtime = min(all_runtimes)
        max_runtime = max(all_runtimes)

        print(
            f"{code_type} runtime stats - Avg: {avg_runtime:.6f}s, Std: {std_runtime:.6f}s, Runs: {len(all_runtimes)}"
        )

        return RuntimeResult(
            success=True,
            avg_runtime=avg_runtime,
            std_runtime=std_runtime,
            min_runtime=min_runtime,
            max_runtime=max_runtime,
            runs_completed=len(all_runtimes),
        )

    def _compile_cpp_program(self, code: str) -> Optional[str]:
        """
        Compile C++ code and return path to executable.

        Args:
            code: Complete C++ program source code

        Returns:
            Path to compiled executable or None if compilation failed
        """
        try:
            # Set environment to avoid warnings
            env = os.environ.copy()
            env["TOKENIZERS_PARALLELISM"] = "false"

            # Create temporary files
            with tempfile.NamedTemporaryFile(mode="w", suffix=".cpp", delete=False) as cpp_file:
                cpp_file.write(code)
                cpp_file_path = cpp_file.name

            exe_file_path = cpp_file_path.replace(".cpp", ".out")

            # Compile with optimizations for more realistic runtime measurement
            compile_cmd = [self.compiler_path, "-std=c++17", "-O2", "-o", exe_file_path, cpp_file_path]

            compile_result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=30, env=env)

            # Cleanup source file
            try:
                os.unlink(cpp_file_path)
            except Exception as e:
                print(f"Error cleaning up source file: {e}")

            if compile_result.returncode != 0:
                print(f"Compilation failed: {compile_result.stderr}")
                return None

            return exe_file_path

        except Exception as e:
            print(f"Compilation error: {e}")
            return None

    def _time_execution(self, exe_path: str) -> Optional[float]:
        """
        Time the execution of a compiled program.

        Args:
            exe_path: Path to the executable

        Returns:
            Runtime in seconds or None if execution failed
        """
        try:
            env = os.environ.copy()
            env["TOKENIZERS_PARALLELISM"] = "false"

            start_time = time.perf_counter()
            result = subprocess.run([exe_path], capture_output=True, text=True, timeout=self.timeout_seconds, env=env)
            end_time = time.perf_counter()

            if result.returncode == 0:
                return end_time - start_time
            else:
                print(f"Execution failed with return code {result.returncode}")
                return None

        except subprocess.TimeoutExpired:
            print("Execution timed out")
            return None
        except Exception as e:
            print(f"Execution error: {e}")
            return None

    def _calculate_efficiency_metrics(
        self, llm_runtime: RuntimeResult, student_runtime: RuntimeResult, llm_correct: bool, student_correct: bool
    ) -> List[Stat]:
        """
        Calculate efficiency comparison metrics between LLM and student code.

        Returns only the three essential metrics:
        1. runtime_efficiency_ratio - LLM runtime / Student runtime
        2. efficiency_alignment_score - How well LLM matches student efficiency
        3. llm_functionally_correct - Whether LLM code passes all tests

        Args:
            llm_runtime: LLM code runtime results
            student_runtime: Student code runtime results
            llm_correct: Whether LLM code is functionally correct
            student_correct: Whether student code is functionally correct

        Returns:
            List of Stat objects with the three essential efficiency metrics
        """
        stats = []

        # Always add LLM correctness indicator
        stats.append(Stat(MetricName("llm_functionally_correct")).add(1.0 if llm_correct else 0.0))

        # Calculate runtime metrics if we have data for both solutions
        if llm_runtime.success and student_runtime.success:
            # Runtime ratio (LLM / Student) - values > 1 mean LLM is slower
            runtime_ratio = (
                llm_runtime.avg_runtime / student_runtime.avg_runtime
                if student_runtime.avg_runtime > 0
                else float("inf")
            )

            # Efficiency alignment score (closer to 1.0 is better alignment)
            # Use reciprocal if LLM is faster to normalize the scale
            if runtime_ratio > 1:
                efficiency_alignment = 1.0 / runtime_ratio
            else:
                efficiency_alignment = runtime_ratio

            print(f"Runtime ratio (LLM/Student): {runtime_ratio:.4f}")
            print(f"Efficiency alignment score: {efficiency_alignment:.4f}")
            print(f"LLM functionally correct: {llm_correct}")

            stats.extend(
                [
                    Stat(MetricName("runtime_efficiency_ratio")).add(runtime_ratio),
                    Stat(MetricName("efficiency_alignment_score")).add(efficiency_alignment),
                ]
            )

        # Handle cases where only one solution has runtime data
        elif llm_runtime.success and not student_runtime.success:
            print("Only LLM runtime available - student solution failed to run")
            stats.extend(
                [
                    Stat(MetricName("runtime_efficiency_ratio")).add(float("inf")),  # LLM runs, student doesn't
                    Stat(MetricName("efficiency_alignment_score")).add(0.0),  # No alignment possible
                ]
            )

        elif not llm_runtime.success and student_runtime.success:
            print("Only student runtime available - LLM solution failed to run")
            stats.extend(
                [
                    Stat(MetricName("runtime_efficiency_ratio")).add(0.0),  # Student runs, LLM doesn't
                    Stat(MetricName("efficiency_alignment_score")).add(0.0),  # No alignment possible
                ]
            )

        else:
            # Neither solution has runtime data
            print("Runtime measurement failed for both solutions")
            stats.extend(
                [
                    Stat(MetricName("runtime_efficiency_ratio")).add(0.0),
                    Stat(MetricName("efficiency_alignment_score")).add(0.0),
                ]
            )

        return stats

    def _extract_student_code(self, model_code: str) -> str:
        """
        Extracts clean C++ code from model output:
        - Removes markdown
        - Trims preambles
        - Removes student's main()
        - Removes out-of-class method definitions
        - Replaces `return NULL;` with `return 0;` for int returns
        """
        import re

        # --- Step 1: Markdown extraction ---
        code_blocks = re.findall(r"```(?:c\+\+)?\n(.*?)```", model_code, flags=re.DOTALL)
        if code_blocks:
            code = "\n".join(code_blocks).strip()
            print("[Markdown extraction] Used fenced code blocks.")
        else:
            # --- Step 2: Trim non-code preamble ---
            lines = model_code.strip().splitlines()
            start_keywords = ("#include", "template", "class", "struct", "void", "int main", "using namespace")
            start_idx = 0
            for i, line in enumerate(lines):
                if any(line.strip().startswith(k) for k in start_keywords):
                    start_idx = i
                    break
            code = "\n".join(lines[start_idx:]).strip()
            print("[Fallback extraction] Trimmed preamble.")

        # --- Step 3: Remove student's main() ---
        main_match = re.search(r"\bint\s+main\s*\([^)]*\)\s*\{", code)
        if main_match:
            code = code[: main_match.start()].strip()
            print("[Code cleaner] Removed student main() function.")

        # --- Step 4: Remove out-of-class method definitions ---
        code = re.sub(r"\bArray\s*<[^>]*>\s*::\s*[\w~]+\s*\([^)]*\)\s*\{[^}]*\}", "", code, flags=re.DOTALL)
        code = re.sub(
            r"template\s*<[^>]*>\s*[\w:<>\s&*]+\s+Array\s*<[^>]*>\s*::\s*[\w~]+\s*\([^)]*\)\s*\{[^}]*\}",
            "",
            code,
            flags=re.DOTALL,
        )

        # --- Step 5: Replace NULL return with 0 ---
        code = re.sub(r"return\s+NULL\s*;", "return 0;", code)

        # --- Final touch ---
        code = code.strip()
        if "print(" in code and "void print()" not in code and "print()" not in code:
            print("⚠️ WARNING: `print()` is called in test input but not defined.")

        print(f"[Final extracted code length] {len(code)}")
        print(f"[Code preview]\n{code[:300]}...\n")
        return code

    def _create_complete_program(self, template: str, student_code: str, test_input: str) -> str:
        """Create a complete C++ program using template, student code, and test input."""
        import re

        print("\n--- Create Complete Program Debug ---")
        print(f"Template length: {len(template)}")
        print(f"Student code length: {len(student_code)}")
        print(f"Test input length: {len(test_input)}")

        # --- Step 1: Clean the raw test input ---
        clean_input = re.sub(r"STD input:\s*$", "", test_input).strip()

        # --- Step 2: Fix mismatches from test input ---
        # Fix incorrect constructor calls: reduce args from 2 to 1
        clean_input = re.sub(r"new\s+Array\s*<([^>]+)>\s*\([^,]+,\s*[^)]+\)", r"new Array<\1>(200)", clean_input)

        # Remove all `print()` calls
        clean_input = re.sub(r"\b\w+->print\(\);\s*", "", clean_input)

        print(f"Cleaned input: '{clean_input}'")

        # --- Step 3: Use template structure ---
        if "{{ STUDENT_ANSWER }}" in template:
            print("Using template with STUDENT_ANSWER placeholder")

            complete_code = template.replace("{{ STUDENT_ANSWER }}", student_code)

            template_loop_pattern = r"{%\s*for\s+TEST\s+in\s+TESTCASES\s*%}.*?{%\s*endfor\s*%}"
            test_block = f"""{{
            {clean_input};
           }}"""

            complete_code = re.sub(template_loop_pattern, test_block, complete_code, flags=re.DOTALL)
            complete_code = re.sub(r"{%.*?%}", "", complete_code, flags=re.DOTALL)
            complete_code = re.sub(r"{{.*?}}", "", complete_code, flags=re.DOTALL)

            print(f"Template-based complete code length: {len(complete_code)}")
            return complete_code

        # --- Step 4: Default fallback structure ---
        print("No template found. Using fallback layout.")
        return f"""#include <iostream>
    #include <vector>
    #include <algorithm>
    using namespace std;

    {student_code}

    int main() {{
        {clean_input};
        return 0;
    }}"""

    def _compile_and_run_cpp(self, code: str) -> Tuple[bool, str, str]:
        """Compile and run C++ code, return (success, stdout, stderr)."""
        print("\n--- Compile and Run (Functional Correctness) ---")
        print(f"Code length: {len(code)}")

        try:
            # Set environment to avoid HuggingFace tokenizer warnings
            env = os.environ.copy()
            env["TOKENIZERS_PARALLELISM"] = "false"

            # Create temporary files
            with tempfile.NamedTemporaryFile(mode="w", suffix=".cpp", delete=False) as cpp_file:
                cpp_file.write(code)
                cpp_file_path = cpp_file.name

            print(f"Created temp file: {cpp_file_path}")

            # Use .out extension instead of .exe for better cross-platform compatibility
            exe_file_path = cpp_file_path.replace(".cpp", ".out")

            # Compile the code
            compile_cmd = [self.compiler_path, "-std=c++17", "-o", exe_file_path, cpp_file_path]
            print(f"Compile command: {' '.join(compile_cmd)}")

            compile_result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=30, env=env)

            print(f"Compile return code: {compile_result.returncode}")
            if compile_result.stderr:
                print(f"Compile stderr: {compile_result.stderr}")

            if compile_result.returncode != 0:
                return False, "", f"Compilation Error: {compile_result.stderr}"

            # Run the executable
            print(f"Running executable: {exe_file_path}")
            run_result = subprocess.run([exe_file_path], capture_output=True, text=True, timeout=10, env=env)

            print(f"Run return code: {run_result.returncode}")
            print(f"Run stdout: '{run_result.stdout}'")

            return True, run_result.stdout.strip(), run_result.stderr.strip()

        except subprocess.TimeoutExpired:
            print("ERROR: Execution timed out")
            return False, "", "Execution timed out"
        except FileNotFoundError as e:
            print(f"ERROR: Compiler not found: {e}")
            return False, "", f"Compiler '{self.compiler_path}' not found. Please install g++ or specify correct path."
        except Exception as e:
            print(f"ERROR: Exception during compilation: {e}")
            return False, "", f"Error: {str(e)}"
        finally:
            # Clean up temporary files
            try:
                if "cpp_file_path" in locals():
                    os.unlink(cpp_file_path)
                    print(f"Cleaned up: {cpp_file_path}")
                if "exe_file_path" in locals() and os.path.exists(exe_file_path):
                    os.unlink(exe_file_path)
                    print(f"Cleaned up: {exe_file_path}")
            except Exception as e:
                print(f"Cleanup error: {e}")

    def _simulate_execution(self, student_code: str, test_case: dict) -> str:
        """
        This method is no longer used since we only support actual compilation.
        Kept for backwards compatibility but should not be called.
        """
        print("WARNING: _simulate_execution called but simulation is not supported")
        return test_case.get("output", "")

    def _create_failure_stats(self, error_message: str) -> List[Stat]:
        """Create default statistics for failure cases."""
        print(f"RUNTIME EFFICIENCY METRIC FAILURE: {error_message}")
        return [
            Stat(MetricName("runtime_efficiency_ratio")).add(0.0),
            Stat(MetricName("efficiency_alignment_score")).add(0.0),
            Stat(MetricName("llm_functionally_correct")).add(0.0),
        ]

    def _create_correctness_failure_stats(self, llm_correct: bool, student_correct: bool) -> List[Stat]:
        """
        This method is no longer used since we always attempt runtime measurement.
        Kept for backwards compatibility.
        """
        print("WARNING: _create_correctness_failure_stats called but should not be used anymore")
        return self._create_failure_stats("Deprecated correctness failure stats")


class CodeEfficiencyMetric(Metric):
    """
    Comprehensive metric combining functional correctness and runtime efficiency evaluation.

    This metric first evaluates functional correctness and then measures runtime efficiency
    alignment between LLM-generated code and student reference code when both are correct.
    """

    def __init__(
        self,
        compile_code: bool = True,
        compiler_path: str = "g++",
        num_runtime_runs: int = 5,
        timeout_seconds: int = 10,
    ):
        """
        Initialize the code efficiency metric.

        Args:
            compile_code: Whether to actually compile and run code (True) or simulate (False)
            compiler_path: Path to the C++ compiler (default: "g++")
            num_runtime_runs: Number of times to run each solution for averaging (default: 5)
            timeout_seconds: Maximum execution time per run (default: 10)
        """
        self.functional_correctness_metric = FunctionalCorrectnessMetric(
            compile_code=compile_code, compiler_path=compiler_path
        )
        self.runtime_efficiency_metric = RuntimeEfficiencyMetric(
            compile_code=compile_code,
            compiler_path=compiler_path,
            num_runtime_runs=num_runtime_runs,
            timeout_seconds=timeout_seconds,
        )

    def evaluate_generation(
        self,
        adapter_spec: AdapterSpec,
        request_state: RequestState,
        metric_service: MetricService,
        eval_cache_path: str,
    ) -> List[Stat]:
        """
        Evaluate both functional correctness and runtime efficiency.

        Returns:
            List of Stat objects containing both functional correctness and efficiency metrics
        """
        stats = []

        # Get functional correctness metrics
        functional_stats = self.functional_correctness_metric.evaluate_generation(
            adapter_spec, request_state, metric_service, eval_cache_path
        )
        stats.extend(functional_stats)

        # Get runtime efficiency metrics
        efficiency_stats = self.runtime_efficiency_metric.evaluate_generation(
            adapter_spec, request_state, metric_service, eval_cache_path
        )
        stats.extend(efficiency_stats)

        return stats
