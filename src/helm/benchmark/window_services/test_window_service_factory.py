from tempfile import TemporaryDirectory
from typing import Dict
from helm.benchmark.window_services.test_utils import get_tokenizer_service

from helm.benchmark.window_services.window_service_factory import WindowServiceFactory
from helm.proxy.models import ALL_MODELS


def test_all_models_have_window_services():
    with TemporaryDirectory() as tmpdir:
        tokenizer_service = get_tokenizer_service(tmpdir)
        for model in ALL_MODELS:
            window_service = WindowServiceFactory.get_window_service(model.name, tokenizer_service)
            assert window_service


# TODO: Delete everything after this after deleting WindowService subclasses for these models.
_INT_MAX: int = 2147483647


_MODEL_NAME_TO_EXPECTED_WINDOW_SERVICE_PROPERTIES = {
    "AlephAlpha/luminous-base": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "",
        "tokenizer_name": "AlephAlpha/luminous-base",
    },
    "AlephAlpha/luminous-extended": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "",
        "tokenizer_name": "AlephAlpha/luminous-extended",
    },
    "AlephAlpha/luminous-supreme": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "",
        "tokenizer_name": "AlephAlpha/luminous-supreme",
    },
    "HuggingFaceM4/idefics-80b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "HuggingFaceM4/idefics-80b",
    },
    "HuggingFaceM4/idefics-80b-instruct": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "HuggingFaceM4/idefics-80b-instruct",
    },
    "HuggingFaceM4/idefics-9b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "HuggingFaceM4/idefics-9b",
    },
    "HuggingFaceM4/idefics-9b-instruct": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "HuggingFaceM4/idefics-9b-instruct",
    },
    "anthropic/claude-2.0": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 8000,
        "max_sequence_and_generated_tokens_length": 9016,
        "max_sequence_length": 8000,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "anthropic/claude",
    },
    "anthropic/claude-instant-v1": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 8000,
        "max_sequence_and_generated_tokens_length": 9016,
        "max_sequence_length": 8000,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "anthropic/claude",
    },
    "anthropic/claude-v1.3": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 8000,
        "max_sequence_and_generated_tokens_length": 9016,
        "max_sequence_length": 8000,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "anthropic/claude",
    },
    "anthropic/stanford-online-all-v4-s3": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 8192,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 8192,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "cohere/command-medium-beta": {
        "end_of_text_token": "",
        "max_request_length": 2020,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2019,
        "prefix_token": ":",
        "tokenizer_name": "cohere/cohere",
    },
    "cohere/command-xlarge-beta": {
        "end_of_text_token": "",
        "max_request_length": 2020,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2019,
        "prefix_token": ":",
        "tokenizer_name": "cohere/cohere",
    },
    "cohere/large-20220720": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2047,
        "prefix_token": ":",
        "tokenizer_name": "cohere/cohere",
    },
    "cohere/medium-20220720": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2047,
        "prefix_token": ":",
        "tokenizer_name": "cohere/cohere",
    },
    "cohere/medium-20221108": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2047,
        "prefix_token": ":",
        "tokenizer_name": "cohere/cohere",
    },
    "cohere/small-20220720": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2047,
        "prefix_token": ":",
        "tokenizer_name": "cohere/cohere",
    },
    "cohere/xlarge-20220609": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2047,
        "prefix_token": ":",
        "tokenizer_name": "cohere/cohere",
    },
    "cohere/xlarge-20221108": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2047,
        "prefix_token": ":",
        "tokenizer_name": "cohere/cohere",
    },
    "databricks/dolly-v2-12b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "databricks/dolly-v2-3b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "databricks/dolly-v2-7b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "eleutherai/pythia-12b-v0": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "eleutherai/pythia-1b-v0": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "eleutherai/pythia-2.8b-v0": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "eleutherai/pythia-6.9b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "google/palm": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "gooseai/gpt-j-6b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-j-6B",
    },
    "gooseai/gpt-neo-20b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "huggingface/gpt-j-6b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-j-6B",
    },
    "huggingface/gpt2": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 1025,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 1024,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "huggingface/santacoder": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "bigcode/santacoder",
    },
    "huggingface/starcoder": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 8192,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 8192,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "bigcode/starcoder",
    },
    "lightningai/lit-gpt": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "lightningai/lit-gpt",
    },
    "lmsys/vicuna-13b-v1.3": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "hf-internal-testing/llama-tokenizer",
    },
    "lmsys/vicuna-7b-v1.3": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "hf-internal-testing/llama-tokenizer",
    },
    "meta/llama-13b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "hf-internal-testing/llama-tokenizer",
    },
    "meta/llama-2-13b": {
        "end_of_text_token": "</s>",
        "max_request_length": 4096,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4096,
        "prefix_token": "<s>",
        "tokenizer_name": "meta-llama/Llama-2-7b-hf",
    },
    "meta/llama-2-70b": {
        "end_of_text_token": "</s>",
        "max_request_length": 4096,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4096,
        "prefix_token": "<s>",
        "tokenizer_name": "meta-llama/Llama-2-7b-hf",
    },
    "meta/llama-2-7b": {
        "end_of_text_token": "</s>",
        "max_request_length": 4096,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4096,
        "prefix_token": "<s>",
        "tokenizer_name": "meta-llama/Llama-2-7b-hf",
    },
    "meta/llama-30b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "hf-internal-testing/llama-tokenizer",
    },
    "meta/llama-65b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "hf-internal-testing/llama-tokenizer",
    },
    "meta/llama-7b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "hf-internal-testing/llama-tokenizer",
    },
    "microsoft/TNLGv2_530B": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2047,
        "prefix_token": "<<",
        "tokenizer_name": "huggingface/gpt2",
    },
    "microsoft/TNLGv2_7B": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2047,
        "prefix_token": "<<",
        "tokenizer_name": "huggingface/gpt2",
    },
    "mistralai/mistral-7b-v0.1": {
        "end_of_text_token": "</s>",
        "max_request_length": 1000000000000000019884624838656,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 1000000000000000019884624838656,
        "prefix_token": "<s>",
        "tokenizer_name": "mistralai/Mistral-7B-v0.1",
    },
    "mosaicml/mpt-30b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "mosaicml/mpt-7b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "mosaicml/mpt-instruct-30b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "mosaicml/mpt-instruct-7b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "neurips/local": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "neurips/local",
    },
    "nvidia/megatron-gpt2": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 1024,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 1024,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/ada": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/babbage": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/code-cushman-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/code-davinci-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/code-davinci-002": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 4001,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4000,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/curie": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/davinci": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/gpt-3.5-turbo-0301": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 4001,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4000,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "openai/cl100k_base",
    },
    "openai/gpt-3.5-turbo-0613": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 4001,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4000,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "openai/cl100k_base",
    },
    "openai/gpt-3.5-turbo-16k-0613": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 16001,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 16000,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "openai/cl100k_base",
    },
    "openai/gpt-4-0314": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/gpt-4-0613": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/gpt-4-32k-0314": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/gpt-4-32k-0613": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-ada-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-babbage-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-curie-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-davinci-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-davinci-002": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 4001,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4000,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-davinci-003": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 4001,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4000,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-embedding-ada-002": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-similarity-ada-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-similarity-babbage-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-similarity-curie-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "openai/text-similarity-davinci-001": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "stabilityai/stablelm-base-alpha-3b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 4097,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4096,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "stabilityai/stablelm-base-alpha-7b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 4097,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 4096,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "stanford/alpaca-7b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<s>",
        "tokenizer_name": "hf-internal-testing/llama-tokenizer",
    },
    "tiiuae/falcon-40b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": None,
        "tokenizer_name": "tiiuae/falcon-7b",
    },
    "tiiuae/falcon-40b-instruct": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": None,
        "tokenizer_name": "tiiuae/falcon-7b",
    },
    "tiiuae/falcon-7b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": None,
        "tokenizer_name": "tiiuae/falcon-7b",
    },
    "tiiuae/falcon-7b-instruct": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": None,
        "tokenizer_name": "tiiuae/falcon-7b",
    },
    "together/bloom": {
        "end_of_text_token": "</s>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "</s>",
        "tokenizer_name": "bigscience/bloom",
    },
    "together/flan-t5-xxl": {
        "end_of_text_token": "</s>",
        "max_request_length": 511,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 511,
        "prefix_token": "",
        "tokenizer_name": "google/flan-t5-xxl",
    },
    "together/glm": {
        "end_of_text_token": "</s>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "",
        "tokenizer_name": "TsinghuaKEG/ice",
    },
    "together/gpt-j-6b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-j-6B",
    },
    "together/gpt-jt-6b-v1": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-j-6B",
    },
    "together/gpt-neox-20b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "together/gpt-neoxt-chat-base-20b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "together/h3-2.7b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 1025,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 1024,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "huggingface/gpt2",
    },
    "together/opt-1.3b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "</s>",
        "tokenizer_name": "facebook/opt-66b",
    },
    "together/opt-175b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "</s>",
        "tokenizer_name": "facebook/opt-66b",
    },
    "together/opt-6.7b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "</s>",
        "tokenizer_name": "facebook/opt-66b",
    },
    "together/opt-66b": {
        "end_of_text_token": "</s>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "</s>",
        "tokenizer_name": "facebook/opt-66b",
    },
    "together/redpajama-incite-base-3b-v1": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "together/redpajama-incite-base-7b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "together/redpajama-incite-instruct-3b-v1": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "together/redpajama-incite-instruct-7b": {
        "end_of_text_token": "<|endoftext|>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "<|endoftext|>",
        "tokenizer_name": "EleutherAI/gpt-neox-20b",
    },
    "together/t0pp": {
        "end_of_text_token": "</s>",
        "max_request_length": 1024,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 1024,
        "prefix_token": "",
        "tokenizer_name": "bigscience/T0pp",
    },
    "together/t5-11b": {
        "end_of_text_token": "</s>",
        "max_request_length": 511,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 511,
        "prefix_token": "",
        "tokenizer_name": "google/t5-11b",
    },
    "together/ul2": {
        "end_of_text_token": "</s>",
        "max_request_length": 511,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 511,
        "prefix_token": "",
        "tokenizer_name": "google/ul2",
    },
    "together/yalm": {
        "end_of_text_token": "</s>",
        "max_request_length": 2049,
        "max_sequence_and_generated_tokens_length": _INT_MAX,
        "max_sequence_length": 2048,
        "prefix_token": "</s>",
        "tokenizer_name": "Yandex/yalm",
    },
    "writer/palmyra-base": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": 2048,
        "max_sequence_length": 2048,
        "prefix_token": "",
        "tokenizer_name": "huggingface/gpt2",
    },
    "writer/palmyra-e": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": 2048,
        "max_sequence_length": 2048,
        "prefix_token": "",
        "tokenizer_name": "huggingface/gpt2",
    },
    "writer/palmyra-instruct-30": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": 2048,
        "max_sequence_length": 2048,
        "prefix_token": "",
        "tokenizer_name": "huggingface/gpt2",
    },
    "writer/palmyra-large": {
        "end_of_text_token": "",
        "max_request_length": 2048,
        "max_sequence_and_generated_tokens_length": 2048,
        "max_sequence_length": 2048,
        "prefix_token": "",
        "tokenizer_name": "huggingface/gpt2",
    },
    "writer/palmyra-x": {
        "end_of_text_token": "",
        "max_request_length": 8192,
        "max_sequence_and_generated_tokens_length": 8192,
        "max_sequence_length": 8192,
        "prefix_token": "",
        "tokenizer_name": "huggingface/gpt2",
    },
    "writer/silk-road": {
        "end_of_text_token": "",
        "max_request_length": 8192,
        "max_sequence_and_generated_tokens_length": 8192,
        "max_sequence_length": 8192,
        "prefix_token": "",
        "tokenizer_name": "huggingface/gpt2",
    },
}


def format_window_service_properties(properties: Dict) -> str:
    kwargs = []
    kwargs.append(("tokenizer_name", f'"{properties["tokenizer_name"]}"'))
    kwargs.append(("max_sequence_length", properties["max_sequence_length"]))
    if properties["max_request_length"] != properties["max_sequence_length"]:
        kwargs.append(("max_request_length", properties["max_request_length"]))
    if properties["max_sequence_and_generated_tokens_length"] != _INT_MAX:
        kwargs.append(
            ("max_sequence_and_generated_tokens_length", properties["max_sequence_and_generated_tokens_length"])
        )
    if properties["end_of_text_token"]:
        kwargs.append(("end_of_text_token", f'"{properties["end_of_text_token"]}"'))
    if properties["prefix_token"]:
        kwargs.append(("prefix_token", f'"{properties["prefix_token"]}"'))
    kwargs_string = ", ".join(f"{key}={value}" for key, value in kwargs)
    return f"DefaultWindowService(service, {kwargs_string})"


def test_window_factory():
    with TemporaryDirectory() as tmpdir:
        window_services = {}
        tokenizer_service = get_tokenizer_service(tmpdir)
        for model_name, expected_window_service_properties in _MODEL_NAME_TO_EXPECTED_WINDOW_SERVICE_PROPERTIES.items():
            window_service = WindowServiceFactory.get_window_service(model_name, tokenizer_service)
            actual_window_service_properties = {
                "tokenizer_name": window_service.tokenizer_name,
                "max_sequence_length": window_service.max_sequence_length,
                "max_request_length": window_service.max_request_length,
                "max_sequence_and_generated_tokens_length": window_service.max_sequence_and_generated_tokens_length,
                "end_of_text_token": window_service.end_of_text_token,
                "prefix_token": window_service.prefix_token,
            }
            window_services[type(window_service).__name__] = format_window_service_properties(
                actual_window_service_properties
            )
            assert actual_window_service_properties == expected_window_service_properties, model_name
        for k, v in window_services.items():
            print(f"{k}: {v}")
